apiVersion: v1
kind: Pod
metadata:
  name: bert-training-job
  namespace: ml-training
  labels:
    # Team attribution
    team: nlp-team

    # Project tracking
    project: language-models

    # Cost center for financial reporting
    cost-center: research-ml

    # Experiment tracking (for ML workflows)
    experiment-id: exp-bert-2024-001

    # Additional custom tags
    model-type: transformer
    gpu-requirement: a100
    priority: high
    workload-type: training

spec:
  restartPolicy: Never
  containers:
    - name: trainer
      image: my-ml-training:latest
      resources:
        requests:
          nvidia.com/gpu: "8"  # Request 8 GPUs
        limits:
          nvidia.com/gpu: "8"
      env:
        - name: EXPERIMENT_ID
          value: "exp-bert-2024-001"
        - name: MODEL_NAME
          value: "bert-large"

---
apiVersion: v1
kind: Pod
metadata:
  name: inference-service
  namespace: inference-prod
  labels:
    team: ml-platform
    project: model-serving
    cost-center: production-ml
    workload-type: inference
    model: bert-base
    service-tier: production

spec:
  containers:
    - name: inference
      image: my-inference-service:latest
      resources:
        requests:
          # Using GPU sharing for inference
          nvidia.com/gpu-shared: "1"
        limits:
          nvidia.com/gpu-shared: "1"
      env:
        - name: MODEL_PATH
          value: "/models/bert-base"
        - name: BATCH_SIZE
          value: "32"

  # Annotations for GPU autoscaler
  annotations:
    gpu-autoscaler.io/sharing-mode: "mps"
    gpu-autoscaler.io/optimize: "true"
