# Default values for gpu-autoscaler

# Controller configuration
controller:
  enabled: true
  replicaCount: 3
  image:
    repository: gpuautoscaler/controller
    pullPolicy: IfNotPresent
    tag: "v0.1.0"

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  leaderElection: true

  prometheusURL: "http://prometheus-operated:9090"

  # Webhook configuration
  webhook:
    enabled: true
    port: 9443
    certManager:
      enabled: true

# DCGM Exporter for GPU metrics
dcgmExporter:
  enabled: true
  image:
    repository: nvcr.io/nvidia/k8s/dcgm-exporter
    pullPolicy: IfNotPresent
    tag: "3.1.8-3.1.5-ubuntu22.04"

  # Run as DaemonSet on GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # DCGM metrics collection interval
  scrapeInterval: 10s

  # Service Monitor for Prometheus
  serviceMonitor:
    enabled: true
    interval: 10s

# Prometheus configuration
prometheus:
  enabled: true

  server:
    persistentVolume:
      enabled: true
      size: 50Gi

    retention: "7d"

    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  # Prometheus Operator
  prometheusOperator:
    enabled: true

# Grafana configuration
grafana:
  enabled: true

  adminPassword: "admin"  # Change in production!

  persistence:
    enabled: true
    size: 10Gi

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Pre-configured dashboards
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'gpu-autoscaler'
        orgId: 1
        folder: 'GPU Autoscaler'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/gpu-autoscaler

  dashboardsConfigMaps:
    gpu-autoscaler: "gpu-autoscaler-dashboards"

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-operated:9090
        access: proxy
        isDefault: true

# Cost tracking (requires TimescaleDB)
cost:
  enabled: false

  timescaledb:
    enabled: false
    # If enabled, deploys TimescaleDB for cost data
    image:
      repository: timescale/timescaledb
      tag: "latest-pg14"

    persistence:
      enabled: true
      size: 20Gi

# Phase 3: Autoscaling Engine configuration
autoscaling:
  enabled: true

  # Cloud provider: aws, gcp, azure
  provider: aws

  # Autoscaler reconciliation interval
  reconcileInterval: 30s

  # Scale-up configuration
  scaleUp:
    # GPU utilization threshold to trigger scale-up (0-1)
    threshold: 0.8
    # Cooldown period after scale-up
    cooldown: 3m
    # Time to wait before scaling up for pending pods
    pendingPodTimeout: 2m

  # Scale-down configuration
  scaleDown:
    # GPU utilization threshold to trigger scale-down (0-1)
    threshold: 0.2
    # Cooldown period after scale-down
    cooldown: 10m
    # Grace period for pod eviction
    gracePeriod: 30s

  # Node pool limits
  limits:
    # Minimum number of GPU nodes
    minNodes: 0
    # Maximum number of GPU nodes
    maxNodes: 100

  # Spot instance orchestration (Phase 3)
  spot:
    enabled: true
    # Target percentage of spot instances (0-1)
    targetPercentage: 0.6
    # Enable automatic spot termination handling
    gracefulEviction: true
    # Check interval for spot termination notices
    checkInterval: 5s
    # Diversify across multiple instance types
    diversify: true

  # Multi-tier scaling (Phase 3)
  multiTier:
    enabled: true
    # Priority order: spot → on-demand → reserved
    # Spot instances for majority of workload
    spotPercentage: 0.6
    # On-demand for stability
    onDemandPercentage: 0.35
    # Reserved instances for baseline (if available)
    reservedPercentage: 0.05

  # Predictive scaling (Phase 3)
  predictive:
    enabled: false
    # Historical lookback period for pattern analysis
    lookbackDays: 7
    # Prediction horizon
    horizonMinutes: 30
    # Minimum confidence level to trigger pre-warming (0-1)
    confidenceThreshold: 0.7
    # Pre-warm nodes when predicted utilization exceeds this (0-1)
    preWarmThreshold: 0.7

  # Node pools configuration
  nodePools:
    # Spot instance pool (primary)
    - name: spot-pool
      enabled: true
      minSize: 0
      maxSize: 50
      gpuType: nvidia-tesla-v100
      instanceTypes:
        - p3.2xlarge  # 1x V100
        - p3.8xlarge  # 4x V100
        - g4dn.xlarge # 1x T4 (diversify)
      capacityType: spot
      spotPercentage: 1.0
      priority: 10
      labels:
        gpu-autoscaler.io/capacity-type: spot
        gpu-autoscaler.io/pool: spot-pool
      availabilityZones:
        - us-west-2a
        - us-west-2b
        - us-west-2c

    # On-demand instance pool (fallback)
    - name: on-demand-pool
      enabled: true
      minSize: 0
      maxSize: 30
      gpuType: nvidia-tesla-v100
      instanceTypes:
        - p3.2xlarge
      capacityType: on-demand
      priority: 5
      labels:
        gpu-autoscaler.io/capacity-type: on-demand
        gpu-autoscaler.io/pool: on-demand-pool
      availabilityZones:
        - us-west-2a
        - us-west-2b

    # Reserved instance pool (baseline)
    - name: reserved-pool
      enabled: false
      minSize: 2
      maxSize: 10
      gpuType: nvidia-tesla-v100
      instanceTypes:
        - p3.2xlarge
      capacityType: reserved
      priority: 1
      labels:
        gpu-autoscaler.io/capacity-type: reserved
        gpu-autoscaler.io/pool: reserved-pool

  # Cloud provider specific configuration
  aws:
    region: us-west-2
    # Auto Scaling Group names (map to node pools)
    autoScalingGroups:
      spot-pool: gpu-spot-asg
      on-demand-pool: gpu-on-demand-asg
      reserved-pool: gpu-reserved-asg

  gcp:
    projectID: ""
    region: us-central1
    # Managed Instance Group names (map to node pools)
    instanceGroups:
      spot-pool: gpu-spot-mig
      on-demand-pool: gpu-on-demand-mig

  azure:
    subscriptionID: ""
    resourceGroup: ""
    region: eastus
    # VM Scale Set names (map to node pools)
    vmScaleSets:
      spot-pool: gpu-spot-vmss
      on-demand-pool: gpu-on-demand-vmss

# Bin-packing configuration (Phase 2)
binPacking:
  enabled: true
  # Packing strategy: bestfit, firstfit, worstfit
  strategy: bestfit
  # Run packing analysis interval
  analysisInterval: 5m
  # Enable automatic workload consolidation
  autoConsolidate: false  # Manual for safety

# GPU sharing configuration (Phase 2)
sharing:
  # NVIDIA MIG support (Multi-Instance GPU)
  mig:
    enabled: false
    # Auto-configure MIG profiles based on workload demand
    autoConfig: true
    # Supported profiles: 1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb
    # For 80GB A100: 1g.10gb, 2g.20gb, 3g.40gb, 4g.40gb, 7g.80gb
    defaultProfile: "1g.5gb"
    # Enable MIG on specific nodes (via node labels)
    nodeSelector:
      nvidia.com/mig.capable: "true"

  # NVIDIA MPS support (Multi-Process Service)
  mps:
    enabled: false
    # Maximum concurrent MPS clients per GPU
    maxClients: 16
    # Default active thread percentage per client
    defaultActiveThreads: 100
    # MPS server resource limits
    resources:
      memoryLimitMB: 1024
    # Enable MPS on specific nodes
    nodeSelector:
      nvidia.com/mps.capable: "true"

  # Time-slicing support
  timeSlicing:
    enabled: false
    # Number of virtual GPUs per physical GPU
    replicasPerGPU: 4
    # Time slice duration in milliseconds
    sliceMs: 100
    # Fairness mode: roundrobin, priority, weighted
    fairnessMode: roundrobin
    # Enable time-slicing on specific nodes
    nodeSelector: {}
    # Allow oversubscription
    oversubscribe: false

# Admission webhook (Phase 2 - Zero-touch optimization)
admissionWebhook:
  enabled: true

  # Rewrite pod GPU requests based on historical usage
  rewriteRequests: true

  # Fail open if webhook is unavailable
  failurePolicy: Ignore

  # Optimization strategies (enabled by webhook)
  optimization:
    # Enable automatic MIG conversion for eligible workloads
    enableMIG: false
    # Enable automatic MPS conversion for inference workloads
    enableMPS: false
    # Enable automatic time-slicing for dev/batch workloads
    enableTimeSlicing: false
    # Workload type detection (auto-detect from labels/annotations)
    autoDetectWorkloadType: true

  # Namespace exemptions (skip optimization for these namespaces)
  exemptNamespaces:
    - kube-system
    - kube-public
    - kube-node-lease

# GPU Sharing Policies (Phase 2 CRDs)
gpuSharingPolicies:
  enabled: true
  # Default policies to create on install
  defaultPolicies:
    # Policy for inference workloads
    - name: inference-mps
      strategy: mps
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: inference
      priority: 10

    # Policy for development workloads
    - name: development-timeslicing
      strategy: timeslicing
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: development
      priority: 5
      timeSlicingConfig:
        replicasPerGPU: 4

    # Policy for small batch workloads
    - name: batch-mig
      strategy: mig
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: batch
      priority: 8
      migConfig:
        autoSelectProfile: true

# RBAC configuration
rbac:
  create: true

# Service Account
serviceAccount:
  create: true
  name: gpu-autoscaler

# Namespace
namespace: gpu-autoscaler-system
