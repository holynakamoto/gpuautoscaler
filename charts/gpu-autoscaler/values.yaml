# Default values for gpu-autoscaler

# Controller configuration
controller:
  enabled: true
  replicaCount: 3
  image:
    repository: gpuautoscaler/controller
    pullPolicy: IfNotPresent
    tag: "v0.1.0"

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  leaderElection: true

  prometheusURL: "http://prometheus-operated:9090"

  # Webhook configuration
  webhook:
    enabled: true
    port: 9443
    certManager:
      enabled: true

# DCGM Exporter for GPU metrics
dcgmExporter:
  enabled: true
  image:
    repository: nvcr.io/nvidia/k8s/dcgm-exporter
    pullPolicy: IfNotPresent
    tag: "3.1.8-3.1.5-ubuntu22.04"

  # Run as DaemonSet on GPU nodes
  nodeSelector:
    nvidia.com/gpu.present: "true"

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # DCGM metrics collection interval
  scrapeInterval: 10s

  # Service Monitor for Prometheus
  serviceMonitor:
    enabled: true
    interval: 10s

# Prometheus configuration
prometheus:
  enabled: true

  server:
    persistentVolume:
      enabled: true
      size: 50Gi

    retention: "7d"

    resources:
      limits:
        cpu: 1000m
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

  # Prometheus Operator
  prometheusOperator:
    enabled: true

# Grafana configuration
grafana:
  enabled: true

  adminPassword: "admin"  # Change in production!

  persistence:
    enabled: true
    size: 10Gi

  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Pre-configured dashboards
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'gpu-autoscaler'
        orgId: 1
        folder: 'GPU Autoscaler'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/gpu-autoscaler

  dashboardsConfigMaps:
    gpu-autoscaler: "gpu-autoscaler-dashboards"

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        url: http://prometheus-operated:9090
        access: proxy
        isDefault: true

# Cost tracking (requires TimescaleDB)
cost:
  enabled: false

  timescaledb:
    enabled: false
    # If enabled, deploys TimescaleDB for cost data
    image:
      repository: timescale/timescaledb
      tag: "latest-pg14"

    persistence:
      enabled: true
      size: 20Gi

# Autoscaling configuration
autoscaling:
  enabled: true

  # Scale-up triggers
  scaleUp:
    # Scale up when pending GPU pods exist for > 2 minutes
    pendingPodThreshold: 2m
    # Scale up when cluster GPU utilization > 80%
    utilizationThreshold: 80

  # Scale-down triggers
  scaleDown:
    # Scale down when node idle for > 10 minutes
    idleThreshold: 10m
    # Scale down when node GPU utilization < 20%
    utilizationThreshold: 20

  # Spot instance configuration
  spot:
    enabled: true
    # Percentage of workloads to run on spot instances
    targetPercentage: 60

# Bin-packing configuration (Phase 2)
binPacking:
  enabled: true
  # Packing strategy: bestfit, firstfit, worstfit
  strategy: bestfit
  # Run packing analysis interval
  analysisInterval: 5m
  # Enable automatic workload consolidation
  autoConsolidate: false  # Manual for safety

# GPU sharing configuration (Phase 2)
sharing:
  # NVIDIA MIG support (Multi-Instance GPU)
  mig:
    enabled: false
    # Auto-configure MIG profiles based on workload demand
    autoConfig: true
    # Supported profiles: 1g.5gb, 2g.10gb, 3g.20gb, 4g.20gb, 7g.40gb
    # For 80GB A100: 1g.10gb, 2g.20gb, 3g.40gb, 4g.40gb, 7g.80gb
    defaultProfile: "1g.5gb"
    # Enable MIG on specific nodes (via node labels)
    nodeSelector:
      nvidia.com/mig.capable: "true"

  # NVIDIA MPS support (Multi-Process Service)
  mps:
    enabled: false
    # Maximum concurrent MPS clients per GPU
    maxClients: 16
    # Default active thread percentage per client
    defaultActiveThreads: 100
    # MPS server resource limits
    resources:
      memoryLimitMB: 1024
    # Enable MPS on specific nodes
    nodeSelector:
      nvidia.com/mps.capable: "true"

  # Time-slicing support
  timeSlicing:
    enabled: false
    # Number of virtual GPUs per physical GPU
    replicasPerGPU: 4
    # Time slice duration in milliseconds
    sliceMs: 100
    # Fairness mode: roundrobin, priority, weighted
    fairnessMode: roundrobin
    # Enable time-slicing on specific nodes
    nodeSelector: {}
    # Allow oversubscription
    oversubscribe: false

# Admission webhook (Phase 2 - Zero-touch optimization)
admissionWebhook:
  enabled: true

  # Rewrite pod GPU requests based on historical usage
  rewriteRequests: true

  # Fail open if webhook is unavailable
  failurePolicy: Ignore

  # Optimization strategies (enabled by webhook)
  optimization:
    # Enable automatic MIG conversion for eligible workloads
    enableMIG: false
    # Enable automatic MPS conversion for inference workloads
    enableMPS: false
    # Enable automatic time-slicing for dev/batch workloads
    enableTimeSlicing: false
    # Workload type detection (auto-detect from labels/annotations)
    autoDetectWorkloadType: true

  # Namespace exemptions (skip optimization for these namespaces)
  exemptNamespaces:
    - kube-system
    - kube-public
    - kube-node-lease

# GPU Sharing Policies (Phase 2 CRDs)
gpuSharingPolicies:
  enabled: true
  # Default policies to create on install
  defaultPolicies:
    # Policy for inference workloads
    - name: inference-mps
      strategy: mps
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: inference
      priority: 10

    # Policy for development workloads
    - name: development-timeslicing
      strategy: timeslicing
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: development
      priority: 5
      timeSlicingConfig:
        replicasPerGPU: 4

    # Policy for small batch workloads
    - name: batch-mig
      strategy: mig
      podSelector:
        matchLabels:
          gpu-autoscaler.io/workload-type: batch
      priority: 8
      migConfig:
        autoSelectProfile: true

# RBAC configuration
rbac:
  create: true

# Service Account
serviceAccount:
  create: true
  name: gpu-autoscaler

# Namespace
namespace: gpu-autoscaler-system
